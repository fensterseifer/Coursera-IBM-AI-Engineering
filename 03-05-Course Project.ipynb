{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Using TensorFlow backend.\n"
                }
            ],
            "source": "import keras\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.30</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age  Strength  \n0            1040.0           676.0   28     79.99  \n1            1055.0           676.0   28     61.89  \n2             932.0           594.0  270     40.27  \n3             932.0           594.0  365     41.05  \n4             978.4           825.5  360     44.30  "
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import pandas as pd\n\nconcrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\nconcrete_data.head()"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(1030, 9)"
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "concrete_data.shape"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "concrete_data_columns = concrete_data.columns\npredictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']]\ntarget = concrete_data['Strength']"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "8"
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "n_cols = predictors_norm.shape[1]\nn_cols"
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.model_selection import train_test_split\nimport statistics as stat\nimport numpy as np"
        },
        {
            "cell_type": "code",
            "execution_count": 94,
            "metadata": {},
            "outputs": [],
            "source": "def regression_model():\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model"
        },
        {
            "cell_type": "code",
            "execution_count": 95,
            "metadata": {},
            "outputs": [],
            "source": "def run(inTimes, inPredictors, inTarget, inEpochs, inRM = regression_model):\n    scores = np.zeros(inTimes)\n\n    for i in range(inTimes):\n        trainPredictors, testPredictors, trainTarget, testTarget = train_test_split(inPredictors, inTarget, test_size=0.3)\n        model = inRM()\n        history = model.fit(trainPredictors, trainTarget, epochs=inEpochs, verbose=0)\n        print('Train {} loss: {}'.format(i, stat.mean(history.history['loss'])))\n        scores[i] = model.evaluate(testPredictors, testTarget, verbose=0)\n    \n    return scores"
        },
        {
            "cell_type": "code",
            "execution_count": 96,
            "metadata": {},
            "outputs": [],
            "source": "def printScores(scores):\n    print('Test scores Mean: {}'.format(stat.mean(scores)))\n    print('Test scores Standard Deviation: {}'.format(stat.stdev(scores)))"
        },
        {
            "cell_type": "code",
            "execution_count": 97,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train 0 loss: 5172.23300212421\nTrain 1 loss: 6178.858210096637\nTrain 2 loss: 4765.258912510125\nTrain 3 loss: 1354.0987489071235\nTrain 4 loss: 4026.9054296536387\nTrain 5 loss: 785.0364295515042\nTrain 6 loss: 313.88814325636866\nTrain 7 loss: 410.55095511141496\nTrain 8 loss: 837.502652814762\nTrain 9 loss: 494.64806918309563\nTrain 10 loss: 409.0893763626035\nTrain 11 loss: 1609.4198979546788\nTrain 12 loss: 31152.16710285668\nTrain 13 loss: 4485.048207494812\nTrain 14 loss: 20311.551405326856\nTrain 15 loss: 8377.008170655312\nTrain 16 loss: 265.32223311478486\nTrain 17 loss: 11035.849527266208\nTrain 18 loss: 11365.204958277689\nTrain 19 loss: 570.9866272643535\nTrain 20 loss: 9085.928158637906\nTrain 21 loss: 3817.2238940448738\nTrain 22 loss: 2985.74284965822\nTrain 23 loss: 12066.04519778438\nTrain 24 loss: 6648.613118374173\nTrain 25 loss: 3248.3212303256855\nTrain 26 loss: 820.6763783613355\nTrain 27 loss: 13771.017919481677\nTrain 28 loss: 454.87531958115744\nTrain 29 loss: 2461.6211756924486\nTrain 30 loss: 2326.0887949789317\nTrain 31 loss: 8697.08611702378\nTrain 32 loss: 34508.66779188011\nTrain 33 loss: 718.4472398437839\nTrain 34 loss: 6641.856496882974\nTrain 35 loss: 21880.75241851595\nTrain 36 loss: 1421.671732311381\nTrain 37 loss: 45451.10916950284\nTrain 38 loss: 287.9076057000233\nTrain 39 loss: 5489.659761609462\nTrain 40 loss: 6871.898036506007\nTrain 41 loss: 387.5053202263228\nTrain 42 loss: 1659.5748694419067\nTrain 43 loss: 20110.411326481877\nTrain 44 loss: 5150.807832171775\nTrain 45 loss: 2744.4602926750026\nTrain 46 loss: 524.6268286103184\nTrain 47 loss: 1327.147559195213\nTrain 48 loss: 3241.1558269997013\nTrain 49 loss: 369.9726327414122\nTest scores Mean: 392.6167835978783\nTest scores Standard Deviation: 406.4073854641039\n"
                }
            ],
            "source": "printScores(run(50, predictors, target, 50))"
        },
        {
            "cell_type": "code",
            "execution_count": 98,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train 0 loss: 940.6572913507681\nTrain 1 loss: 1078.892310608164\nTrain 2 loss: 857.0094765299931\nTrain 3 loss: 758.5361486621704\nTrain 4 loss: 869.516672621051\nTrain 5 loss: 831.6264414810837\nTrain 6 loss: 837.8775778913564\nTrain 7 loss: 1022.049116784888\nTrain 8 loss: 963.6582257046217\nTrain 9 loss: 889.0923463810301\nTrain 10 loss: 928.1824428618665\nTrain 11 loss: 1262.6060385356168\nTrain 12 loss: 1027.444348891175\nTrain 13 loss: 795.0624847869238\nTrain 14 loss: 1047.966616202472\nTrain 15 loss: 953.6220868741061\nTrain 16 loss: 990.7748748271376\nTrain 17 loss: 1020.3626957890991\nTrain 18 loss: 990.8629709653815\nTrain 19 loss: 1015.0115789964228\nTrain 20 loss: 975.8081074009656\nTrain 21 loss: 766.2582215883464\nTrain 22 loss: 1001.7171761234193\nTrain 23 loss: 997.8281253360743\nTrain 24 loss: 951.6726151837522\nTrain 25 loss: 1064.5314692169486\nTrain 26 loss: 914.5760083261773\nTrain 27 loss: 915.2407631019878\nTrain 28 loss: 888.7840057830176\nTrain 29 loss: 848.897681090716\nTrain 30 loss: 954.2081874742653\nTrain 31 loss: 897.6643859833653\nTrain 32 loss: 750.2814762444569\nTrain 33 loss: 900.2007282281418\nTrain 34 loss: 985.2326054130481\nTrain 35 loss: 1033.9837902476486\nTrain 36 loss: 837.2763693630943\nTrain 37 loss: 935.006862872543\nTrain 38 loss: 758.1986852199856\nTrain 39 loss: 1028.2234372756682\nTrain 40 loss: 1010.6760504163088\nTrain 41 loss: 1039.8629118527056\nTrain 42 loss: 1014.3037435070784\nTrain 43 loss: 949.402802772469\nTrain 44 loss: 909.1574934876081\nTrain 45 loss: 900.8543249968848\nTrain 46 loss: 933.573398219094\nTrain 47 loss: 934.4986316671385\nTrain 48 loss: 986.4915481080625\nTrain 49 loss: 920.603677575565\nTest scores Mean: 359.18744967760006\nTest scores Standard Deviation: 95.5818940857825\n"
                }
            ],
            "source": "predictors_norm = (predictors - predictors.mean()) / predictors.std()\nprintScores(run(50, predictors_norm, target, 50))"
        },
        {
            "cell_type": "code",
            "execution_count": 99,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train 0 loss: 497.20185611582\nTrain 1 loss: 818.0750606699294\nTrain 2 loss: 749.654751278458\nTrain 3 loss: 588.7119354970775\nTrain 4 loss: 687.0876613892067\nTrain 5 loss: 541.3923615749534\nTrain 6 loss: 773.7988428674029\nTrain 7 loss: 618.5815682475478\nTrain 8 loss: 547.5363806050759\nTrain 9 loss: 554.5797557186386\nTrain 10 loss: 770.2685388635432\nTrain 11 loss: 629.1439864963103\nTrain 12 loss: 479.54463910678226\nTrain 13 loss: 650.9007374295513\nTrain 14 loss: 545.0236869909363\nTrain 15 loss: 627.3849378755651\nTrain 16 loss: 520.6656214557706\nTrain 17 loss: 532.9272262264786\nTrain 18 loss: 538.4086840687512\nTrain 19 loss: 556.0134713981089\nTrain 20 loss: 696.9518360746387\nTrain 21 loss: 545.8907017298578\nTrain 22 loss: 529.6796940248782\nTrain 23 loss: 642.6936910361027\nTrain 24 loss: 557.083638617083\nTrain 25 loss: 546.717323576865\nTrain 26 loss: 622.3137181291303\nTrain 27 loss: 560.3588371528699\nTrain 28 loss: 535.498193080336\nTrain 29 loss: 505.2446285276373\nTrain 30 loss: 645.2151424339178\nTrain 31 loss: 606.9254059033652\nTrain 32 loss: 602.1277297545074\nTrain 33 loss: 551.7212197679157\nTrain 34 loss: 629.1090770856352\nTrain 35 loss: 584.2292937210232\nTrain 36 loss: 563.5480850451465\nTrain 37 loss: 634.9413597488403\nTrain 38 loss: 482.4328545059014\nTrain 39 loss: 593.4636465713353\nTrain 40 loss: 667.5582373562203\nTrain 41 loss: 677.3006532318285\nTrain 42 loss: 503.4901724022007\nTrain 43 loss: 552.8711843308066\nTrain 44 loss: 505.93912338082237\nTrain 45 loss: 552.8113621745327\nTrain 46 loss: 504.7627889969147\nTrain 47 loss: 777.6212486288254\nTrain 48 loss: 557.3034547337546\nTrain 49 loss: 641.487356036288\nTest scores Mean: 168.71325026823865\nTest scores Standard Deviation: 24.613756806903282\n"
                }
            ],
            "source": "printScores(run(50, predictors_norm, target, 100))"
        },
        {
            "cell_type": "code",
            "execution_count": 100,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train 0 loss: 391.7027591524376\nTrain 1 loss: 402.5329551947563\nTrain 2 loss: 391.1433929937524\nTrain 3 loss: 395.88783617313294\nTrain 4 loss: 351.0828809814876\nTrain 5 loss: 406.18651407334414\nTrain 6 loss: 331.639540254167\nTrain 7 loss: 382.2401379373368\nTrain 8 loss: 347.70525821013854\nTrain 9 loss: 331.41886798811026\nTrain 10 loss: 373.553005907691\nTrain 11 loss: 340.3585492480909\nTrain 12 loss: 357.02811297772496\nTrain 13 loss: 321.9226405256168\nTrain 14 loss: 387.3176553484405\nTrain 15 loss: 303.1520163960662\nTrain 16 loss: 357.89414331497\nTrain 17 loss: 336.19890177902664\nTrain 18 loss: 331.4728876903814\nTrain 19 loss: 387.39410211496977\nTrain 20 loss: 381.5516392953055\nTrain 21 loss: 342.3307672097977\nTrain 22 loss: 321.48075756385157\nTrain 23 loss: 322.10796850778524\nTrain 24 loss: 342.67761596700853\nTrain 25 loss: 350.77881924241655\nTrain 26 loss: 362.0738410291526\nTrain 27 loss: 378.6779626642616\nTrain 28 loss: 333.53526078519144\nTrain 29 loss: 343.3945607749103\nTrain 30 loss: 318.9365617081725\nTrain 31 loss: 430.1766978430252\nTrain 32 loss: 380.91235954887827\nTrain 33 loss: 384.8210301919585\nTrain 34 loss: 334.75480785692616\nTrain 35 loss: 415.98592151827023\nTrain 36 loss: 325.2125118089483\nTrain 37 loss: 383.5252867760308\nTrain 38 loss: 390.66456826039393\nTrain 39 loss: 331.69240016222994\nTrain 40 loss: 385.3160168230583\nTrain 41 loss: 335.6965133730482\nTrain 42 loss: 378.93730411185635\nTrain 43 loss: 348.7503189960961\nTrain 44 loss: 386.86401814729265\nTrain 45 loss: 378.73655927650145\nTrain 46 loss: 370.50005920071544\nTrain 47 loss: 398.4080803321701\nTrain 48 loss: 447.67944121573737\nTrain 49 loss: 376.06958042583915\nTest scores Mean: 126.74132382241653\nTest scores Standard Deviation: 14.161833608329168\n"
                }
            ],
            "source": "def regression_model2():\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model\n\nprintScores(run(50, predictors_norm, target, 50, regression_model2))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}